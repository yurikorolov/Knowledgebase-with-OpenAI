# Knowledgebase with OpenAI

**Knowledgebase with OpenAI** is Retrieval-Augmented Generation (RAG) system based on vector search. It combines vectorized Q&A data with OpenAI's GPT-4 to build an intelligent question-answering platform. It uses vector similarity search to retrieve relevant answers from a knowledge base and generates natural language responses using GPT-4 as context.

## Project Overview

This project integrates vector search, leveraging the `pgvector` extension for PostgreSQL to perform similarity searches on vectorized text, and OpenAI GPT-4 API for generating human-like answers to user queries. The Q&A data is processed and stored as embeddings, which are vectorized representations of text, making it efficient to search for similar items.

### Retrieval-Augmented Generation (RAG) Concept

The RAG approach allows the system to retrieve the most relevant context (in this case, Q&A pairs) using a vector search engine. The retrieved context is then passed to a generative model (GPT-4) to create the final response, providing more accurate, contextually rich answers. This approach combines the best of search and generation-based systems.

## Application Flow

1. **Data Vectorization**:
    - Q&A pairs from a JSON file are vectorized using the model which is configurable via an environment variable (`MODEL_NAME`). This process generates embeddings that represent the meaning of the text.
    - The embeddings are stored in PostgreSQL's `pgvector` extension for efficient similarity search.

2. **Search Flow**:
    - When a user submits a question, the system vectorizes the query using the same model, then searches for the top 3 most similar Q&A pairs in the database.
    - The relevant answers are retrieved and passed as context to the OpenAI GPT-4 API, which generates a more comprehensive and natural response.

3. **Queue Management for OpenAI Rate Limits**:
    - To avoid exceeding OpenAI's rate limits (10,000 tokens per minute, 3 requests per minute, 200 requests per day), requests are managed using asynchronous queues. If limits are reached, requests are queued and processed later, and the user can query the status with a unique `queue_id`.

## File Structure

```
├── app
│   ├── alembic/                    # Folder for Alembic migrations
│   │   ├── alembic.ini             # Alembic configuration file
│   │   ├── env.py                  # Alembic environment setup for migrations
│   │   └── versions/               # Stores database migration versions
│   ├── alembic.ini                 # Duplicate alembic config for root access
│   ├── config.py                   # Configuration file for the FastAPI app
│   ├── __init__.py                 # Initializes app module
│   ├── main.py                     # Main FastAPI application with routes for Q&A and vectorization
│   ├── requirements.txt            # Python dependencies
│   └── vectorization.py            # Script for vectorizing content and adding it to the database
├── data.json                       # Example JSON file to be vectorized and added to the DB
├── docker-compose-gpu.yml          # Docker Compose for GPU version of the project
├── docker-compose.yml              # Main Docker Compose for CPU-based version
├── Dockerfile                      # Dockerfile for CPU-based FastAPI app
├── Dockerfile.gpu                  # Dockerfile for GPU-based FastAPI app
├── entrypoint.sh                   # Entry script to wait for DB before starting app
├── .env.example                    # Example environment configuration file
├── LICENSE                         # License file for the project
├── logs/                           # Log files storage
│   ├── app/                        # Logs generated by the FastAPI app
│   └── nginx/                      # Logs generated by Nginx server
├── model_cache.py                  # Script to pre-cache the transformer model
├── nginx/                          # Nginx configuration files and logs
│   ├── conf.d/                     # Nginx site-specific configuration
│   │   └── default.conf            # Default Nginx server configuration
│   ├── Dockerfile                  # Dockerfile to build the Nginx container
│   ├── entrypoint.sh               # Nginx entrypoint script to manage SSL setup
│   ├── logs/                       # Log storage for Nginx access and error logs
│   │   ├── access.log              # Access log for Nginx
│   │   └── error.log               # Error log for Nginx
│   ├── nginx.conf                  # Main Nginx configuration file
│   └── ssl/                        # SSL certificates directory (LetsEncrypt or Snakeoil)
├── README.md                       # Project README file
└── volumes/                        # Persistent volumes for data and configs
    ├── app/                        # App's persistent data storage
    ├── letsencrypt/                # LetsEncrypt certificate storage (production)
    ├── logs/                       # General logs storage
    ├── nginx/                      # Nginx configuration and cert storage
    │   └── certs/                  # Nginx SSL certificates
    ├── redis/                      # Redis cache directory (if needed)
    ├── ssl/                        # SSL storage for development (Snakeoil certs)
    └── transformers_cache/         # Model cache to avoid downloading each time
```

## Usage

### JSON File Format for Q&A Import

The JSON file containing Q&A pairs should have the following format:

```json
[
    {
        "question": "What is the corporate tax rate?",
        "answer": "The corporate tax rate is 30%."
    },
    {
        "question": "What is the income tax for individuals?",
        "answer": "The income tax rate for individuals depends on their income bracket."
    }
]
```

### Request Format

- **Search Request**: Submit a question to get a response from the knowledge base and OpenAI.

  ```json
  {
    "question": "What is the corporate tax rate?"
  }
  ```

- **Queued Requests**: If the request is queued due to rate limits, a `queue_id` will be returned. You can query the status using:

  ```json
  {
    "queue_id": "12345"
  }
  ```

### Running the Application

1. **Clone the repository**:

    ```bash
    git clone https://github.com/yurikorolov/Knowledgebase-with-OpenAI.git
    cd Knowledgebase-with-OpenAI
    ```

2. **Set up the environment variables**:
   Create an `.env` file based on `.env.example` and set the following variables:

   ```bash
   MODEL_NAME=DeepPavlov/rubert-base-cased-conversational
   TRANSFORMERS_CACHE=/app/volumes/transformers_cache
   OPENAI_API_KEY=<your-openai-api-key>
   DB_USER=postgres
   DB_PASSWORD=db_password
   DB_NAME=db_name
   DB_HOST=db    # This should point to the database container name
   DB_PORT=5432
   IS_PROD=true  # Use false for development
   DOMAIN_NAME=<your-domain>
   ```

3. **Build and Run with Docker**:
    To run the app locally:

    ```bash
    docker-compose up --build
    ```

    To run with GPU (assuming Docker and NVIDIA runtime are installed):

    ```bash
    docker-compose -f docker-compose-gpu.yml up --build
    ```

    If running on production with LetsEncrypt certificates, set `IS_PROD=true` in `.env`. For development, snakeoil SSL certificates are generated instead.

4. **Run Migrations**:
   Before starting the application, run Alembic migrations to set up the database schema:

    ```bash
    docker exec -it app alembic upgrade head
    ```

5. **Import Q&A Data**:
   Use the vectorization script to import and vectorize a JSON file of Q&A pairs:

    ```bash
    docker exec -it app python3 app/vectorization.py --file /path/to/your/qa.json
    ```

### Model Cache

To prevent downloading the Huggingface model every time the container is running, the model is cached in `volumes/transformers_cache`. This ensures faster startup times and prevents repeated downloads.

### Model Used for Vectorization

The model used for vectorization in this project is configurable via an environment variable (`MODEL_NAME`). By default, the project uses the **DeepPavlov/rubert-base-cased-conversational** model for vectorizing Russian text. However, other models can be easily substituted by changing the `MODEL_NAME` in the `.env` file.

Additionally, the location where models are cached during downloading can be specified via an environment variable (`TRANSFORMERS_CACHE`). This ensures the model is not redownloaded every time the Docker container is restarted. You can customize both of these values in the `.env` file.

#### Environment Variables:

- `MODEL_NAME`: The model to use for vectorization. Default is `DeepPavlov/rubert-base-cased-conversational`.
- `TRANSFORMERS_CACHE`: Directory where models will be cached for reuse.

Example `.env` file:

```bash
MODEL_NAME=DeepPavlov/rubert-base-cased-conversational
TRANSFORMERS_CACHE=/app/volumes/transformers_cache
```

#### List of Other Models You Can Use

Below is a list of other models that can be used, depending on your language or task. Simply update the `MODEL_NAME` environment variable in the `.env` file to use one of the following models.

##### **Multilingual Models**:

- **[sentence-transformers/paraphrase-multilingual-mpnet-base-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2)**
  Suitable for 100+ languages.

- **[sentence-transformers/distiluse-base-multilingual-cased-v2](https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2)**
  Supports 50+ languages.

##### **English Models**:

- **[sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)**
- **[bert-base-uncased](https://huggingface.co/bert-base-uncased)**

##### **Portuguese Model**:

- **[neuralmind/bert-base-portuguese-cased](https://huggingface.co/neuralmind/bert-base-portuguese-cased)**
  A cased BERT model for general-purpose NLP in Portuguese.

For the full list of supported models and how to change them, refer to the official [Hugging Face Model Hub](https://huggingface.co/models).

## Nginx Configuration

The Nginx container acts as a reverse proxy to the FastAPI app, enabling SSL with Let's Encrypt or Snakeoil certificates (for development). The configuration is automatically set based on the `.env` file, with `DOMAIN_NAME` and `IS_PROD` variables controlling whether real SSL certificates are used.

### LetsEncrypt Setup

If `IS_PROD=true`, Nginx automatically requests and renews SSL certificates using LetsEncrypt.

For development, if `IS_PROD=false`, Nginx uses Snakeoil self-signed certificates for SSL.

## Deploying to Production

1. Make sure to set `IS_PROD=true` and provide the correct `DOMAIN_NAME` in `.env`.
2. Rebuild and run the app:
   ```bash
   docker-compose up --build -d
   ```

3. Check the Nginx logs for LetsEncrypt certificate generation:
   ```bash
   docker logs nginx
   ```

## Hardware Requirements

- **CPU Deployment**: Running the model and vectorization on CPU works, but response times may be slower, especially with large Q&A datasets.
- **GPU Deployment**: If available, running the model on a GPU will significantly speed up the vectorization and search processes.

## Alternatives to OpenAI API

This app uses OpenAI GPT-4 to generate answers, but you can easily modify it to use any other transformer-based generative models. However, keep in mind that the OpenAI API rate limits (10,000 TPM, 3 RPM, 200 RPD) are handled via an asynchronous queue system.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
